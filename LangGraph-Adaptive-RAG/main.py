import streamlit as st
from graph.app import app
from langchain_teddynote.messages import stream_graph
from langchain_core.runnables import RunnableConfig
import uuid

st.set_page_config(page_title="LangGraph RAG", page_icon="ğŸ“š")
st.header("LangGraph RAG ğŸ“š")

# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)
config = RunnableConfig(recursion_limit=20, configurable={"thread_id": uuid.uuid4()})

# Streamlit UI
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if question := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": question})
    with st.chat_message("user"):
        st.markdown(question)

    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            inputs = {"question": question}
            response = app.invoke(inputs, config)

            # app.invokeì˜ ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¼ ê²½ìš° 'answer' í‚¤ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜,
            # ì „ì²´ ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œí•©ë‹ˆë‹¤.
            if isinstance(response, dict) and "generation" in response:
                full_response = response["generation"]
            else:
                full_response = str(response)  # ë‹¤ë¥¸ í˜•íƒœì˜ ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜

            st.markdown(full_response)
            st.session_state.messages.append(
                {"role": "assistant", "content": full_response}
            )
